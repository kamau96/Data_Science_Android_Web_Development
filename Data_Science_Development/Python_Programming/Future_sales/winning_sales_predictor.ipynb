{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a30afd-ab8e-4c8c-b868-d3f7e282ecb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../../../Data/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650401ea-39bd-4553-868f-06d9ccd2862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows viewing of entire column width of a dataframe \n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f510a57-685d-4082-877f-c4c3460ba1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e3f4f-76db-4f82-aa92-f46decd0a216",
   "metadata": {},
   "source": [
    "### Below are some utility functions that will be used throughout the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c65a1-11d0-4b7f-8127-60f55021bc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n",
    "    \"\"\" \n",
    "    Iterates through all the columns of a dataframe and downcasts the data type\n",
    "     to reduce memory usage. Can also factorize categorical columns to integer dtype.\n",
    "    \"\"\"\n",
    "    def _downcast_numeric(series, allow_categorical=allow_categorical):\n",
    "        \"\"\"\n",
    "        Downcast a numeric series into either the smallest possible int dtype or a specified float dtype.\n",
    "        \"\"\"\n",
    "        if pd.api.types.is_sparse(series.dtype) is True:\n",
    "            return series\n",
    "        elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n",
    "            if pd.api.types.is_datetime64_any_dtype(series.dtype):\n",
    "                return series\n",
    "            else:\n",
    "                if allow_categorical:\n",
    "                    return series\n",
    "                else:\n",
    "                    codes, uniques = series.factorize()\n",
    "                    series = pd.Series(data=codes, index=series.index)\n",
    "                    series = _downcast_numeric(series)\n",
    "                    return series\n",
    "        else:\n",
    "            series = pd.to_numeric(series, downcast=\"integer\")\n",
    "        if pd.api.types.is_float_dtype(series.dtype):\n",
    "            series = series.astype(float_dtype)\n",
    "        return series\n",
    "\n",
    "    if silent is False:\n",
    "        start_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(f\"Memory usage of dataframe is: {start_mem:.2f} MB\")\n",
    "    if df.ndim == 1:\n",
    "        df = _downcast_numeric(df)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            df.loc[:, col] = _downcast_numeric(df.loc[:,col])\n",
    "    if silent is False:\n",
    "        end_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        print(f\"Decreased by {(100 * (start_mem - end_mem) / start_mem):.1f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def shrink_mem_new_cols(matrix, oldcols=None, allow_categorical=False):\n",
    "    # Calls reduce_mem_usage on columns which have not yet been optimized\n",
    "    if oldcols is not None:\n",
    "        newcols = matrix.columns.difference(oldcols)\n",
    "    else:\n",
    "        newcols = matrix.columns\n",
    "    matrix.loc[:,newcols] = reduce_mem_usage(matrix.loc[:,newcols], allow_categorical=allow_categorical)\n",
    "    oldcols = matrix.columns  # This is used to track which columns have already been downcast\n",
    "    return matrix, oldcols\n",
    "\n",
    "\n",
    "def list_if_not(s, dtype=str):\n",
    "    # Puts a variable in a list if it is not already a list\n",
    "    if type(s) not in (dtype, list):\n",
    "        raise TypeError\n",
    "    if (s != \"\") & (type(s) is not list):\n",
    "        s = [s]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65be0a-8b13-4fec-ac19-7db5a302ef6b",
   "metadata": {},
   "source": [
    "### Load the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c35a0f-e934-410b-a3ed-e1912b4b792e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=\"../../../Data/future_sales/\"\n",
    "items = pd.read_csv(path+\"items.csv\")\n",
    "shops = pd.read_csv(path+\"shops.csv\")\n",
    "train = pd.read_csv(path+\"sales_train.csv\")\n",
    "test = pd.read_csv(path+\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0027a-96a6-4f3b-a741-87bedf7734ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{'_'*70}\")\n",
    "display(items.head())\n",
    "display(items.shape)\n",
    "print(f\"{'_'*70}\")\n",
    "display(shops.head())\n",
    "display(shops.shape)\n",
    "print(f\"{'_'*70}\")\n",
    "display(train.head())\n",
    "display(train.shape)\n",
    "print(f\"{'_'*70}\")\n",
    "display(test.head())\n",
    "display(test.shape)\n",
    "print(f\"{'_'*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15027b-189d-49fd-a1ca-6b8296a243ba",
   "metadata": {},
   "source": [
    "### Convert date column to datetime to allow date operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefa103-7894-4cf5-a58f-f846cd490231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891b623-ca99-4c8e-a227-57f386521009",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- Minor data cleaning of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63a07c-6d56-4100-8d1a-587839e080b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge some duplicate shops\n",
    "train[\"shop_id\"] = train[\"shop_id\"].replace({0: 57, 1: 58, 11: 10, 40: 39})\n",
    "# Keep only shops that are in the test set\n",
    "train = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]\n",
    "# Drop training items with extreme or negative prices or sales counts\n",
    "train = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]\n",
    "train = train[(train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f71ab-5111-4ed9-ac45-f988aa801a3a",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e12018-7bc3-4e7f-8deb-895330116a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_testlike_train(sales_train, test=None):\n",
    "    \"\"\"\n",
    "    This function attempts to convert train dataset to a format that resembles how test dataset \n",
    "    looks like i.e test dataset appears to be a cartesian product shops with every combination\n",
    "    of item.\n",
    "    \"\"\"\n",
    "    indexlist = []\n",
    "    # This for loop does a cartesian product of every combination of shop and item each month\n",
    "    for i in sales_train.date_block_num.unique():\n",
    "        x = itertools.product(\n",
    "            [i],\n",
    "            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n",
    "            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n",
    "        )\n",
    "        indexlist.append(np.array(list(x)))\n",
    "    df = pd.DataFrame(\n",
    "        data=np.concatenate(indexlist, axis=0),\n",
    "        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    # Add revenue column to sales_train\n",
    "    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n",
    "    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n",
    "    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n",
    "        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n",
    "        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n",
    "    )\n",
    "\n",
    "    # Merge the grouped data with the index\n",
    "    df = df.merge(\n",
    "        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    if test is not None:\n",
    "        test[\"date_block_num\"] = 34\n",
    "        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
    "        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
    "        test[\"item_id\"] = test.item_id.astype(np.int16)\n",
    "        test = test.drop(\"ID\",axis=1)\n",
    "        \n",
    "        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n",
    "\n",
    "    # Fill empty item_cnt entries with 0\n",
    "    df.item_cnt_month = df.item_cnt_month.fillna(0)\n",
    "    df.item_revenue_month = df.item_revenue_month.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e37062-0fb2-4dca-b8e4-0c4e0172e244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = create_testlike_train(train, test)\n",
    "del(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2bb38-a1f3-46dc-9648-008b44655913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = reduce_mem_usage(matrix, silent=False)\n",
    "oldcols = matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc01b78-225f-4f85-a741-bbca6d1722c9",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Predictor columns are added to the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e1037-c779-4d04-99c1-b5987b370a76",
   "metadata": {},
   "source": [
    "### Item name groups with fuzzywuzzy\n",
    "Items in the items table are ordered alphabetically according to the item_name field, so that similar items are generally listed next to each other. For example, the first two items in the table below are the same game \"Fuse\" for two different consoles, followed by two different licensing options for the same internet security program. This ordering can be used to help group related items together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da03704-912a-49f7-bbf6-d18b0413f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "items.query(\"item_id>3564\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cb1b8-d100-4e4b-a5d3-1ba0d6f06104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "# def add_item_name_groups(matrix, train, items, sim_thresh, feature_name=\"item_name_group\"):\n",
    "#     def partialmatchgroups(items, sim_thresh=sim_thresh):\n",
    "#         def strip_brackets(string):\n",
    "#             string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "#             string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "#             return string\n",
    "\n",
    "#         items = items.copy()\n",
    "#         items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "#         items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "\n",
    "#         def partialcompare(s):\n",
    "#             return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "\n",
    "#         items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "#         # Assign groups\n",
    "#         grp = 0\n",
    "#         for i in range(items.shape[0]):\n",
    "#             items.loc[i, \"partialmatchgroup\"] = grp\n",
    "#             if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "#                 grp += 1\n",
    "#         items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "#         return items\n",
    "\n",
    "#     items = partialmatchgroups(items)\n",
    "#     items = items.rename(columns={\"partialmatchgroup\": feature_name})\n",
    "#     items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n",
    "\n",
    "#     items[feature_name] = items[feature_name].apply(str)\n",
    "#     items[feature_name] = items[feature_name].factorize()[0]\n",
    "#     matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "#     train = train.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "#     return matrix, train\n",
    "\n",
    "\n",
    "# matrix, train = add_item_name_groups(matrix, train, items, 65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
