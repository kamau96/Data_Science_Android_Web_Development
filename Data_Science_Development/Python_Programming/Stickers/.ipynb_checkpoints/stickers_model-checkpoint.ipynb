{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77ecae-7f5c-4b29-ac2e-db8562abcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear (and not quite) models on time series\n",
    "Often, in my job, I have to build models with fast, good, cheap as my only guiding principle. That means that some of these models will never be considered \"production ready\" as they demand too much time for data preparation (as in SARIMA) or require frequent re-training on new data (again, SARIMA) or are difficult to tune (good example - SARIMA). Therefore, it's very often much easier to select a few features from the existing time series and build a simple linear regression model or, say, a random forest. It is good and cheap.\n",
    "\n",
    "This approach is not backed by theory and breaks several assumptions (e.g. Gauss-Markov theorem, especially for errors being uncorrelated), but it is very useful in practice and is often used in machine learning competitions.\n",
    "\n",
    "Feature exctraction\n",
    "The model needs features, and all we have is a 1-dimentional time series. What features can we exctract?\n",
    "\n",
    "Lags of time series\n",
    "Window statistics:\n",
    "Max/min value of series in a window\n",
    "Average/median value in a window\n",
    "Window variance\n",
    "etc.\n",
    "Date and time features:\n",
    "Minute of an hour, hour of a day, day of the week, and so on\n",
    "Is this day a holiday? Maybe there is a special event? Represent that as a boolean feature\n",
    "Target encoding\n",
    "Forecasts from other models (note that we can lose the speed of prediction this way)\n",
    "Let's run through some of the methods and see what we can extract from our ads time series data.\n",
    "\n",
    "Lags of time series\n",
    "Shifting the series n\n",
    " steps back, we get a feature column where the current value of time series is aligned with its value at time tâˆ’n\n",
    ". If we make a 1 lag shift and train a model on that feature, the model will be able to forecast 1 step ahead from having observed the current state of the series. Increasing the lag, say, up to 6, will allow the model to make predictions 6 steps ahead; however it will use data observed 6 steps back. If something fundamentally changes the series during that unobserved period, the model will not catch these changes and will return forecasts with a large error. Therefore, during the initial lag selection, one has to find a balance between the optimal prediction quality and the length of the forecasting horizon.\n",
    "\n",
    "# Creating a copy of the initial datagrame to make various transformations \n",
    "data = pd.DataFrame(ads.Ads.copy())\n",
    "data.columns = [\"y\"]\n",
    "# Adding the lag of the target variable from 6 steps back up to 24\n",
    "for i in range(6, 25):\n",
    "    data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "# take a look at the new dataframe \n",
    "data.tail(7)\n",
    "y\tlag_6\tlag_7\tlag_8\tlag_9\tlag_10\tlag_11\tlag_12\tlag_13\tlag_14\tlag_15\tlag_16\tlag_17\tlag_18\tlag_19\tlag_20\tlag_21\tlag_22\tlag_23\tlag_24\n",
    "Time\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "2017-09-21 17:00:00\t151790\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\t123945.0\t142425.0\t146215.0\t139515.0\n",
    "2017-09-21 18:00:00\t155665\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\t123945.0\t142425.0\t146215.0\n",
    "2017-09-21 19:00:00\t155890\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\t123945.0\t142425.0\n",
    "2017-09-21 20:00:00\t123395\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\t123945.0\n",
    "2017-09-21 21:00:00\t103080\t146020.0\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\n",
    "2017-09-21 22:00:00\t95155\t152120.0\t146020.0\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\n",
    "2017-09-21 23:00:00\t80285\t151790.0\t152120.0\t146020.0\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\n",
    "Great, we have generated a dataset here. Why don't we now train a model?\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# for time-series cross-validation set 5 folds \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "def timeseries_train_test_split(X, y, test_size):\n",
    "    \"\"\"\n",
    "        Perform train-test split with respect to time series structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the index after which test set starts\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    \n",
    "    X_train = X.iloc[:test_index]\n",
    "    y_train = y.iloc[:test_index]\n",
    "    X_test = X.iloc[test_index:]\n",
    "    y_test = y.iloc[test_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)\n",
    "\n",
    "# reserve 30% of data for testing\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "# machine learning in two lines\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
    "         normalize=False)\n",
    "def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        Plots modelled vs fact values, prediction intervals and anomalies\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
    "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
    "    \n",
    "    if plot_intervals:\n",
    "        cv = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=tscv, \n",
    "                                    scoring=\"neg_mean_absolute_error\")\n",
    "        mae = cv.mean() * (-1)\n",
    "        deviation = cv.std()\n",
    "        \n",
    "        scale = 1.96\n",
    "        lower = prediction - (mae + scale * deviation)\n",
    "        upper = prediction + (mae + scale * deviation)\n",
    "        \n",
    "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
    "        plt.plot(upper, \"r--\", alpha=0.5)\n",
    "        \n",
    "        if plot_anomalies:\n",
    "            anomalies = np.array([np.NaN]*len(y_test))\n",
    "            anomalies[y_test<lower] = y_test[y_test<lower]\n",
    "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
    "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    error = mean_absolute_percentage_error(prediction, y_test)\n",
    "    plt.title(\"Mean absolute percentage error {0:.2f}%\".format(error))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True);\n",
    "    \n",
    "def plotCoefficients(model):\n",
    "    \"\"\"\n",
    "        Plots sorted coefficient values of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
    "    coefs.columns = [\"coef\"]\n",
    "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
    "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    coefs.coef.plot(kind='bar')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');\n",
    "plotModelResults(lr, plot_intervals=True)\n",
    "plotCoefficients(lr)\n",
    "\n",
    "\n",
    "Simple lags and linear regression gave us predictions that are not that far off from SARIMA in terms of quality. There are many unnecessary features, so we'll do feature selection in a little while. For now, let's continue engineering!\n",
    "\n",
    "We'll add hour, day of week, and a boolean for is_weekend. To do so, we need to transform the current dataframe index into the datetime format and extract hour and weekday.\n",
    "\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data[\"hour\"] = data.index.hour\n",
    "data[\"weekday\"] = data.index.weekday\n",
    "data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "data.tail()\n",
    "y\tlag_6\tlag_7\tlag_8\tlag_9\tlag_10\tlag_11\tlag_12\tlag_13\tlag_14\tlag_15\tlag_16\tlag_17\tlag_18\tlag_19\tlag_20\tlag_21\tlag_22\tlag_23\tlag_24\thour\tweekday\tis_weekend\n",
    "Time\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "2017-09-21 19:00:00\t155890\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\t123945.0\t142425.0\t19\t3\t0\n",
    "2017-09-21 20:00:00\t123395\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\t123945.0\t20\t3\t0\n",
    "2017-09-21 21:00:00\t103080\t146020.0\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t101360.0\t21\t3\t0\n",
    "2017-09-21 22:00:00\t95155\t152120.0\t146020.0\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t88170.0\t22\t3\t0\n",
    "2017-09-21 23:00:00\t80285\t151790.0\t152120.0\t146020.0\t142815.0\t141995.0\t146630.0\t132335.0\t114380.0\t105635.0\t98860.0\t97290.0\t106495.0\t113950.0\t121910.0\t94945.0\t80195.0\t72150.0\t70335.0\t76050.0\t23\t3\t0\n",
    "We can visualize the resulting features.\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.title(\"Encoded features\")\n",
    "data.hour.plot()\n",
    "data.weekday.plot()\n",
    "data.is_weekend.plot()\n",
    "plt.grid(True);\n",
    "\n",
    "Since we now have different scales in our variables, thousands for the lag features and tens for categorical, we need to transform them into same scale for exploring feature importance and, later, regularization.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "y = data.dropna().y\n",
    "X = data.dropna().drop(['y'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\n",
    "plotCoefficients(lr)\n",
    "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  return self.partial_fit(X, y)\n",
    "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  return self.fit(X, **fit_params).transform(X)\n",
    "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  import sys\n",
    "\n",
    "\n",
    "The test error goes down a little bit. Judging by the coefficients plot, we can say that weekday and is_weekend are useful features.\n",
    "\n",
    "Target encoding\n",
    "I'd like to add another variant for encoding categorical variables: encoding by mean value. If it is undesirable to explode a dataset by using many dummy variables that can lead to the loss of information and if they cannot be used as real values because of the conflicts like \"0 hours < 23 hours\", then it's possible to encode a variable with slightly more interpretable values. The natural idea is to encode with the mean value of the target variable. In our example, every day of the week and every hour of the day can be encoded by the corresponding average number of ads watched during that day or hour. It's very important to make sure that the mean value is calculated over the training set only (or over the current cross-validation fold only) so that the model is not aware of the future.\n",
    "\n",
    "def code_mean(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where keys are unique categories of the cat_feature,\n",
    "    and values are means over real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())\n",
    "Let's look at the averages by hour.\n",
    "\n",
    "average_hour = code_mean(data, 'hour', \"y\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title(\"Hour averages\")\n",
    "pd.DataFrame.from_dict(average_hour, orient='index')[0].plot()\n",
    "plt.grid(True);\n",
    "\n",
    "Finally, let's put all the transformations together in a single function .\n",
    "\n",
    "def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n",
    "    \"\"\"\n",
    "        series: pd.DataFrame\n",
    "            dataframe with timeseries\n",
    "\n",
    "        lag_start: int\n",
    "            initial step back in time to slice target variable \n",
    "            example - lag_start = 1 means that the model \n",
    "                      will see yesterday's values to predict today\n",
    "\n",
    "        lag_end: int\n",
    "            final step back in time to slice target variable\n",
    "            example - lag_end = 4 means that the model \n",
    "                      will see up to 4 days back in time to predict today\n",
    "\n",
    "        test_size: float\n",
    "            size of the test dataset after train/test split as percentage of dataset\n",
    "\n",
    "        target_encoding: boolean\n",
    "            if True - add target averages to the dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # copy of the initial dataset\n",
    "    data = pd.DataFrame(series.copy())\n",
    "    data.columns = [\"y\"]\n",
    "    \n",
    "    # lags of series\n",
    "    for i in range(lag_start, lag_end):\n",
    "        data[\"lag_{}\".format(i)] = data.y.shift(i)\n",
    "    \n",
    "    # datetime features\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5,6])*1\n",
    "    \n",
    "    if target_encoding:\n",
    "        # calculate averages on train set only\n",
    "        test_index = int(len(data.dropna())*(1-test_size))\n",
    "        data['weekday_average'] = list(map(code_mean(data[:test_index], 'weekday', \"y\").get, data.weekday))\n",
    "        data[\"hour_average\"] = list(map(code_mean(data[:test_index], 'hour', \"y\").get, data.hour))\n",
    "\n",
    "        # frop encoded variables \n",
    "        data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "    \n",
    "    # train-test split\n",
    "    y = data.dropna().y\n",
    "    X = data.dropna().drop(['y'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=True)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(lr)\n",
    "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  return self.partial_fit(X, y)\n",
    "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  return self.fit(X, **fit_params).transform(X)\n",
    "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  after removing the cwd from sys.path.\n",
    "\n",
    "\n",
    "We see some overfitting! Hour_average was so great in the training dataset that the model decided to concentrate all of its forces on it. As a result, the quality of prediction dropped. This problem can be solved in a variety of ways; for example, we can calculate the target encoding not for the whole train set, but for some window instead. That way, encodings from the last observed window will most likely better describe the current series state. Alternatively, we can just drop it manually since we are sure that it makes things only worse in this case.\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "prepareData(ads.Ads, lag_start=6, lag_end=25, test_size=0.3, target_encoding=False)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  return self.partial_fit(X, y)\n",
    "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  return self.fit(X, **fit_params).transform(X)\n",
    "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
    "  after removing the cwd from sys.path.\n",
    "Regularization and feature selection\n",
    "As we already know, not all features are equally healthy -- some may lead to overfitting while others should be removed. Besides manual inspection, we can apply regularization. Two of the most popular regression models with regularization are Ridge and Lasso regressions. They both add some more constrains to our loss function.\n",
    "\n",
    "In the case of Ridge regression, those constraints are the sum of squares of the coefficients multiplied by the regularization coefficient. The bigger the coefficient a feature has, the bigger our loss will be. Hence, we will try to optimize the model while keeping the coefficients fairly low.\n",
    "\n",
    "As a result of this L2\n",
    " regularization, we will have higher bias and lower variance, so the model will generalize better (at least that's what we hope will happen).\n",
    "\n",
    "The second regression model, Lasso regression, adds to the loss function, not squares, but absolute values of the coefficients. As a result, during the optimization process, coefficients of unimportant features may become zeroes, which allows for automated feature selection. This regularization type is called L1\n",
    ".\n",
    "\n",
    "First, let's make sure that we have features to drop and that the data has highly correlated features.\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_train.corr());\n",
    "\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "\n",
    "ridge = RidgeCV(cv=tscv)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(ridge, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(ridge)\n",
    "\n",
    "\n",
    "We can clearly see some coefficients are getting closer and closer to zero (though they never actually reach it) as their importance in the model drops.\n",
    "\n",
    "lasso = LassoCV(cv=tscv)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "plotModelResults(lasso, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "plotCoefficients(lasso)\n",
    "\n",
    "\n",
    "Lasso regression turned out to be more conservative; it removed 23-rd lag from the most important features and dropped 5 features completely, which only made the quality of prediction better.\n",
    "\n",
    "BoostingÂ¶\n",
    "Why shouldn't we try XGBoost now?\n",
    "\n",
    "from xgboost import XGBRegressor \n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
    "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "       subsample=1)\n",
    "plotModelResults(xgb, \n",
    "                 X_train=X_train_scaled, \n",
    "                 X_test=X_test_scaled, \n",
    "                 plot_intervals=True, plot_anomalies=True)\n",
    "\n",
    "We have a winner! This is the smallest error on the test set among all the models we've tried so far.\n",
    "\n",
    "But, this victory is decieving, and it might not be the brightest idea to fit xgboost as soon as you get your hands on time series data. Generally, tree-based models handle trends in data poorly when compared with linear models. In that case, you would have to detrend your series first or use some tricks to make the magic happen. Ideally, you can make the series stationary and then use XGBoost. For example, you can forecast trend separately with a linear model and then add predictions from xgboost to get a final forecast.\n",
    "\n",
    "Conclusion\n",
    "We discussed different time series analysis and prediction methods. Unfortunately, or maybe luckily, there is no one way to solve these kind of problems. Methods developed in the 1960s (and some even in the beginning of the 21st century) are still popular, along with LSTMs and RNNs (not covered in this article). This is partially related to the fact that the prediction task, like any other data-related task, requires creativity in so many aspects and definitely requires research. In spite of the large number of formal quality metrics and approaches to parameters estimation, it is often necessary to try something different for each time series. Last but not least, the balance between quality and cost is important. As a good example, the SARIMA model can produce spectacular results after tuning but can require many hours of tambourine dancing time series manipulation while a simple linear regression model can be built in 10 minutes and can achieve more or less comparable results.\n",
    "\n",
    "Useful resources\n",
    "Online textbook for the advanced statistical forecasting course at Duke University - covers various smoothing techniques in detail along with linear and ARIMA models\n",
    "Comparison of ARIMA and Random Forest time series models for prediction of avian influenza H5N1 outbreaks - one of a few cases where using random forest for time series forecasting is actively defended\n",
    "Time Series Analysis (TSA) in Python - Linear Models to GARCH - applying the ARIMA models family to the task of modeling financial indicators (by Brian Christopher)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
